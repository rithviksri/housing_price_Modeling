import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error

# Step 1: Load Dataset
data_path = '/Users/rithviksrinivas/Documents/COMP562/Final-Project/train.csv'
try:
    data = pd.read_csv(data_path)
    print("Dataset successfully loaded.")
except FileNotFoundError:
    print(f"File not found at path: {data_path}")
    raise

# Step 2: Initial Exploration
print("\nDataset Overview:\n", data.head())
print("\nSummary Statistics:\n", data.describe())
print("\nMissing Values:\n", data.isnull().sum())

# Step 3: Data Cleaning
# Drop columns with too many missing values or irrelevant features
irrelevant_columns = ['Alley', 'PoolQC', 'Fence', 'MiscFeature']
data = data.drop(columns=irrelevant_columns, errors='ignore')

# Handle missing values
# Fill missing numerical values with the median
numerical_columns = data.select_dtypes(include=['float64', 'int64']).columns
for column in numerical_columns:
    data[column].fillna(data[column].median(), inplace=True)

# Fill missing categorical values with the mode
categorical_columns = data.select_dtypes(include=['object']).columns
for column in categorical_columns:
    data[column].fillna(data[column].mode()[0], inplace=True)

# Convert categorical variables to dummy variables
data = pd.get_dummies(data, drop_first=True)

# Step 4: Feature Selection
target = 'SalePrice'
if target not in data.columns:
    raise ValueError(f"Target column '{target}' not found in the dataset.")

y = data[target]
X = data.drop(columns=[target])

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 5: Model Training
# Linear Regression Model
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

# Random Forest Regressor
rf_model = RandomForestRegressor(random_state=42, n_estimators=100)
rf_model.fit(X_train, y_train)

# Step 6: Model Evaluation
# Linear Regression Predictions
lr_predictions = lr_model.predict(X_test)
lr_mae = mean_absolute_error(y_test, lr_predictions)
lr_rmse = np.sqrt(mean_squared_error(y_test, lr_predictions))

# Random Forest Predictions
rf_predictions = rf_model.predict(X_test)
rf_mae = mean_absolute_error(y_test, rf_predictions)
rf_rmse = np.sqrt(mean_squared_error(y_test, rf_predictions))

# Display model performance
print("\nLinear Regression Performance:")
print(f"Mean Absolute Error (MAE): {lr_mae:.2f}")
print(f"Root Mean Squared Error (RMSE): {lr_rmse:.2f}")

print("\nRandom Forest Performance:")
print(f"Mean Absolute Error (MAE): {rf_mae:.2f}")
print(f"Root Mean Squared Error (RMSE): {rf_rmse:.2f}")

# Step 7: Visualization
feature_importance = pd.Series(rf_model.feature_importances_, index=X.columns).sort_values(ascending=False)

plt.figure(figsize=(10, 6))
feature_importance.head(10).plot(kind='bar')
plt.title('Top 10 Feature Importances')
plt.ylabel('Importance')
plt.xlabel('Features')
plt.show()




